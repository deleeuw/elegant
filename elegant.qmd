---
title: Squared Distance Scaling
author: 
      name: Jan de Leeuw
      orcid: 0000-0003-1420-1797
      email: jan@deleeuwpdx.net
      affiliation: 
        - name: University of California Los Angeles
          city: Los Angeles
          state: CA
          url: www.ucla.edu
      license: "CC0"
date: last-modified
date-format: long
bibliography: [mypubs.bib, total.bib]
number-sections: true
pdf-engine: lualatex
keep-tex: true
format:
   pdf:
    fontsize: 12pt
    include-in-header: 
     - preamble.tex
    keep-tex: true
    link-citations: true
    documentclass: scrartcl
    number-sections: true
   html:
    fontsize: 12pt
    include-in-header: 
     - preamble.css
    keep-md: true
    number-sections: true
toc: true
toc-depth: 3
editor: source
papersize: letter
graphics: true
link-citations: true
mainfont: Times New Roman
abstract: TBD
---

```{r loadpackages, echo = FALSE}
suppressPackageStartupMessages(library(knitr, quietly = TRUE))
suppressPackageStartupMessages(library(tinytex, quietly = TRUE))
```

**Note:** This is a working manuscript which will be expanded/updated frequently. All suggestions for improvement are welcome. All Rmd, tex, html, pdf, R, and C files are in the public domain. Attribution will be appreciated, but is not required. The files can be found at <https://github.com/deleeuw/elegant>

\sectionbreak

# Introduction

In *squared distance multidimensional scaling* we minimize the least squares loss function \begin{equation}
\sigma(X):=\sum_{k=1}^m w_k(\delta_k^2-d_k^2(X))^2\label{eq-sstress}
\end{equation} over $n\times p$ configurations $X$. Here the $\delta_k$ are known non-negative *pseudo-distances*, the $w_k$ are known positive *weights*, and the $d_k(X)$ are Euclidean distances. Each index $k$ in \eqref{eq-sstress} corresponds with a pair of indices $(i,j)$, with both $1\leq i\leq n$ and $1\leq j\leq n$. Thus we try to find a configuration of $n$ points on $\mathbb{R}^p$ such that the distances between the points approximate the corresponding pseudo-distances in the data.

Loss function \eqref{eq-sstress} is traditionally known as *sstress*. In the *ALSCAL* program for squared distance scaling (@takane_young_deleeuw_A_77) a coordinate descent algorithm is proposed to minimize loss, in which each iteration cycle consists of minimizing $np$ univariate quartics. There have been quite a few alternative algorithms proposed, both in the area of multidimensional scaling (@deleeuw_U_75b, @browne_87, @kearsley_tapia_trosset_94) and in that of low-rank distance matrix completion (@mishra_meyer_sepulchre_11).

The reference section of this paper does not have publication information on @deleeuw_U_75b, in fact not even a URL, because the paper somehow got lost in the folds of time (@takane_16). Proof of its existence are references to it in @takane_77 and @browne_87.
At the time it was concluded that the algorithm proposed in
@deleeuw_U_75b, which was proudly baptized as *ELEGANT*, converged too slowly to be practical. Recent attempts to revive and improve it are @deleeuw_groenen_pietersz_E_16m and @deleeuw_E_16o. This paper is another such attempt.

\sectionbreak

# Majorization

The original derivation of the algorithm in @deleeuw_U_75b was based on *augmentation*. The derivation is reviewed in @deleeuw_groenen_pietersz_E_16m. But improvements are possible
if we discuss it in the general framework of majorization
(@deleeuw_C_96), currently more familiarly known as MM (@lange_).

We change variables from $X$ to $C=XX'$. Thus
$$
\sigma(C)=\sum_{k=1}^m w_k(\delta_k^2-\text{tr}\ A_kC)^2
$$ 
Define 
$$
B:=\sum_{k=1}^m w_k\delta_k^2A_k,
$$ 
and 
$$
V:=\sum_{k=1}^m w_ka_ka_k'
$$ 
with $a_k=\text{vec}(A_k)$. Then 
$$
\sigma(c)=K-2b'c+c'Vc,
$$ 
with $c=\text{vec}(C)$ and $b=\text{vec}(B)$. 

To start the majorization, use $c=\tilde c+(c-\tilde c)$. Then
$$
\sigma(c)=\sigma(\tilde c)-2(c-\tilde c)'(b-V\tilde c)+(c-\tilde c)'V(c-\tilde c),
$$ 
and thus 
$$
\sigma(c)\leq\sigma(\tilde c)-2(c-\tilde c)'(b-V\tilde c)+\lambda(c-\tilde c)'(c-\tilde c)=
\sigma(\tilde c)+\lambda((c-\tilde c)-g)'((c-\tilde c)-g)-\lambda g'g,
$$ 
with $\lambda$ the largest eigenvalue of $V$ and $g:=\lambda^{-1}(b-V\tilde c)$. 

In a majozation step we minimize 
$(c-\overline c)'(c-\overline c),$
where $$\overline{c}:=\tilde c+\lambda^{-1}(b-V\tilde c)$$. 

Taking the inverse of vec show that we must minimize 
$$
\text{tr}\ (\overline{C}-XX')^2
$$
over $X$.

\sectionbreak

# Details

Computing $\lambda$ is simplified by noting that the largest eigenvalue of $V$ is equal to the largest eigenvalue of $W^\frac12HW^\frac12$, where $H$ has elements $$
h_{kl}=a_k'a_l^{\ }.
$$ 
The elements of $H$ are non-negative. Also $h_{kl}$ is equal to 4 if $k=l$ and equal to 1 if $A_k$ and $A_l$ have one index in common, otherwise it is zero. It follows that in the complete case, with $m=n(n-1)/2$, and in addition, if there are unit weights, $\lambda=2n$. In the incomplete case, still with unit weights, $\lambda\leq 2n$.

$$
V\tilde c=\sum_{k=1}^m w_ka_ka_k'\tilde c=\sum_{k=1}^m w_ka_k\text{tr}\ A_k\tilde C=\sum_{k=1}^m w_ka_kd_k^2(\tilde C)
$$ and thus $$
\text{vec}^{-1}(B-V\tilde c)=\sum_{k=1}^m w_k(\delta_k^2-d_k^2(\tilde C)A_k
$$

If it is too expensive to calculate the largest eigenvalue, we can use the bound $$
\lambda=\max_x
\frac{x'W^\frac12HW^\frac12x}{x'x}=
\max_x\frac{x'W^\frac12HW^\frac12x}{x'Wx}\frac{x'Wx}{x'x}\leq 2n\max_k w_k
$$ This is a major improvement ofthe bound that is us3ed, either explicitly or implicitly, in @deleeuw_75 and @deleeuw_groenen_pietersz, which is $$
\lambda\leq\text{tr}\ W^\frac12HW^\frac12 = 4\sum_{k-1}^m w_k.
$$

\sectionbreak

# References
